# ============================================================
#Licensed to the Apache Software Foundation (ASF) under one
#or more contributor license agreements.  See the NOTICE file
#distributed with this work for additional information
#regarding copyright ownership.  The ASF licenses this file
#to you under the Apache License, Version 2.0 (the
#"License"); you may not use this file except in compliance
#with the License.  You may obtain a copy of the License at

#    http://www.apache.org/licenses/LICENSE-2.0

#Unless required by applicable law or agreed to in writing, software
#distributed under the License is distributed on an "AS IS" BASIS,
#WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#See the License for the specific language governing permissions and
#limitations under the License.
# ============================================================

# gemini_api.py

from __future__ import annotations

import os
import io
import sys
import json
import base64
import queue
import threading
import wave
import platform
import random
import time  # ì¶”ê°€
from dataclasses import dataclass
from datetime import datetime
from typing import Optional, Callable

try:
    from dotenv import load_dotenv
    if os.path.exists(".env.local"):
        load_dotenv(dotenv_path=".env.local")
    else:
        load_dotenv()
except Exception:
    pass

import numpy as np
import sounddevice as sd
from pynput import keyboard
import google.generativeai as genai
import requests

IS_WINDOWS = (platform.system() == "Windows")

def _get_env(name: str, default: str | None = None) -> str | None:
    v = os.environ.get(name)
    if v is None or not str(v).strip():
        return default
    return str(v).strip()

def _find_input_device_by_name(name_substr: str) -> int | None:
    if not name_substr: return None
    key = name_substr.lower()
    try:
        for i, d in enumerate(sd.query_devices()):
            if d.get('max_input_channels', 0) > 0 and key in d.get('name', '').lower():
                return i
    except Exception:
        pass
    return None

SAMPLE_RATE = int(_get_env("SAMPLE_RATE", "16000"))
CHANNELS = int(_get_env("CHANNELS", "1"))
DTYPE = _get_env("DTYPE", "int16")
MODEL_NAME = _get_env("MODEL_NAME", "gemini-2.5-flash")
PROMPT_TEXT = (
    "ë‹¤ìŒì€ ì‚¬ìš©ìì˜ í•œêµ­ì–´ ìŒì„±ì…ë‹ˆë‹¤. ì •í™•í•œ ìµœì¢… ì „ì‚¬ë§Œ ì¶œë ¥í•˜ì„¸ìš”."
    " ê·œì¹™: (1) ì‚¬ëŒ ë°œí™”ë§Œ, (2) ë°°ê²½ìŒ/ì¤‘ì–¼ê±°ë¦¼/ë¹„ì–¸ì–´ìŒì€ ì‚­ì œ,"
    " (3) ì¢…ê²°ì–´ë¯¸Â·ë„ì–´ì“°ê¸°Â·ë¬¸ì¥ë¶€í˜¸ë¥¼ ìì—°ìŠ¤ëŸ½ê²Œ, (4) ê¸°í˜¸ë‚˜ ì² ìê°€ í—·ê°ˆë¦¬ë©´ ì˜ë¯¸ê°€ ëª…í™•í•œ í‘œí˜„ìœ¼ë¡œ,"
    " (5) 'ì¶¤', 'ê·¸ë§Œ' ê°™ì€ ì§€ì‹œì–´ëŠ” ê·¸ëŒ€ë¡œ ë³´ì¡´. ì˜¤ì§ í…ìŠ¤íŠ¸ë§Œ ì¶œë ¥."
)
SYSTEM_INSTRUCTION = _get_env(
    "SYSTEM_INSTRUCTION",
    "ë„ˆëŠ” ê³µê° ì„œë¹„ìŠ¤ ë¡œë´‡ 'ëª¨í‹°'ì•¼. í•œêµ­ì–´ë¡œ 1~2ë¬¸ì¥, ë”°ëœ»í•˜ê³  ê°„ê²°í•˜ê²Œ ë‹µí•´."
    " ì‚¬ìš©ìì˜ ì •ì„œ ì‹ í˜¸(í”¼ê³¤, ìŠ¤íŠ¸ë ˆìŠ¤, ë¶ˆì•ˆ)ë¥¼ ë°˜ì˜í•´ ê³µê°í•˜ê³ ,"
    " ì‚¬ì‹¤ì´ ë¶ˆí™•ì‹¤í•˜ë©´ ì§§ê²Œ í™•ì¸ ì§ˆë¬¸ì„ í•´. ê³¼ì¥Â·ê°€ìŠ¤ë¼ì´íŒ… ê¸ˆì§€."
)
TTS_RATE = int(_get_env("TTS_RATE", "0"))
TTS_VOLUME = int(_get_env("TTS_VOLUME", "100"))
TTS_FORCE_VOICE_ID = _get_env("TTS_FORCE_VOICE_ID", "")
TTS_OUTPUT_DEVICE = _get_env("TTS_OUTPUT_DEVICE", "")
GREETING_TEXT = _get_env("GREETING_TEXT", "ì•ˆë…•í•˜ì„¸ìš”! ëª¨í‹°ì…ë‹ˆë‹¤.")
FAREWELL_TEXT = _get_env("FAREWELL_TEXT", "ë„ì›€ì´ ë˜ì—ˆê¸¸ ë°”ë¼ìš”. ì–¸ì œë“  ë‹¤ì‹œ ë¶ˆëŸ¬ì£¼ì„¸ìš”.")
ENABLE_GREETING = _get_env("ENABLE_GREETING", "1") not in ("0", "false", "False")

def _extract_text(resp) -> str:
    t = getattr(resp, "text", None)
    if t and str(t).strip():
        return str(t).strip()
    try:
        pieces = []
        for c in getattr(resp, "candidates", []) or []:
            content = getattr(c, "content", None)
            if not content:
                continue
            for p in getattr(content, "parts", []) or []:
                pt = getattr(p, "text", None)
                if pt and str(pt).strip():
                    pieces.append(str(pt).strip())
        if pieces:
            return "\n".join(pieces).strip()
    except Exception:
        pass
    try:
        return str(resp).strip()
    except Exception:
        return ""

@dataclass
class RecorderState:
    recording: bool = False
    frames_q: queue.Queue = queue.Queue()
    stream: sd.InputStream | None = None

class SapiTTSWorker:
    def __init__(self):
        self._q: queue.Queue[str | None] = queue.Queue()
        self.voice_id: str | None = None
        self.output_device_desc: str | None = None
        self.ready = threading.Event()
        self.thread = threading.Thread(target=self._run, daemon=False)
    def start(self):
        self.thread.start()
        self.ready.wait(timeout=5)
    def speak(self, text: str):
        if not text: return
        print(f"ğŸ”Š TTS enqueue ({len(text)} chars)")
        self._q.put(text)
    def close_and_join(self, drain: bool = True, timeout: float = 15.0):
        try:
            if drain:
                print("â³ TTS ëŒ€ê¸°: í ë¹„ìš°ëŠ” ì¤‘...")
                self._q.join()
            self._q.put(None)
            self.thread.join(timeout=timeout)
        except Exception: pass
    def _run(self):
        pc = None; w32 = None
        try:
            if not IS_WINDOWS:
                print("â„¹ï¸ SAPIëŠ” Windows ì „ìš©ì…ë‹ˆë‹¤. (macOSì—ì„œëŠ” ë¹„í™œì„±)"); self.ready.set(); return
            import pythoncom as pc
            import win32com.client as w32
            pc.CoInitialize()
            voice = w32.Dispatch("SAPI.SpVoice")
            voices = voice.GetVoices()
            chosen_voice_id = None
            if TTS_FORCE_VOICE_ID:
                for i in range(voices.Count):
                    v = voices.Item(i)
                    if v.Id == TTS_FORCE_VOICE_ID: chosen_voice_id = v.Id; break
                if not chosen_voice_id: print(f"â„¹ï¸ TTS_FORCE_VOICE_IDë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤: {TTS_FORCE_VOICE_ID}")
            if not chosen_voice_id:
                for i in range(voices.Count):
                    v = voices.Item(i)
                    blob = f"{v.Id} {v.GetDescription()}".lower()
                    if any(t in blob for t in ["ko", "korean", "í•œêµ­ì–´"]): chosen_voice_id = v.Id; break
                if not chosen_voice_id and voices.Count > 0: chosen_voice_id = voices.Item(0).Id
            if chosen_voice_id:
                for i in range(voices.Count):
                    v = voices.Item(i)
                    if v.Id == chosen_voice_id: voice.Voice = v; self.voice_id = v.Id; break
            outs = voice.GetAudioOutputs()
            chosen_out_desc = None
            if TTS_OUTPUT_DEVICE:
                key = TTS_OUTPUT_DEVICE.lower()
                for i in range(outs.Count):
                    o = outs.Item(i); desc = o.GetDescription()
                    if key in desc.lower(): voice.AudioOutput = o; chosen_out_desc = desc; break
                if not chosen_out_desc: print(f"â„¹ï¸ ì§€ì •í•œ ì¶œë ¥ ì¥ì¹˜ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤: {TTS_OUTPUT_DEVICE}")
            if not chosen_out_desc and outs.Count > 0:
                try: desc = outs.Item(0).GetDescription()
                except Exception: desc = "System Default"
                chosen_out_desc = desc
            self.output_device_desc = chosen_out_desc
            try: voice.Rate = max(-10, min(10, TTS_RATE))
            except Exception: pass
            try: voice.Volume = max(0, min(100, TTS_VOLUME))
            except Exception: pass
            print("ğŸ§ ì‚¬ìš© ê°€ëŠ¥í•œ ìŒì„± ëª©ë¡ (SAPI):")
            for i in range(voices.Count): v = voices.Item(i); print(f"  - [{i}] id='{v.Id}', desc='{v.GetDescription()}'")
            print("ğŸ”‰ ì‚¬ìš© ê°€ëŠ¥í•œ ì¶œë ¥ ì¥ì¹˜ (SAPI):")
            for i in range(outs.Count): o = outs.Item(i); print(f"  - [{i}] '{o.GetDescription()}'")
            print(f"â–¶ ì„ íƒëœ ìŒì„± id='{self.voice_id}'")
            print(f"â–¶ ì„ íƒëœ ì¶œë ¥='{self.output_device_desc}'")
            self.ready.set()
            voice.Speak("T T Sê°€ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤.")
            while True:
                item = self._q.get()
                if item is None: self._q.task_done(); break
                try:
                    print("ğŸ”ˆ TTS speaking..."); voice.Speak(item); print("âœ… TTS done")
                finally: self._q.task_done()
        except Exception as e: print(f"â„¹ï¸ TTS ìŠ¤ë ˆë“œ ì˜¤ë¥˜: {e}"); self.ready.set()
        finally:
            try:
                if pc is not None: pc.CoUninitialize()
            except Exception: pass

class TypecastTTSWorker:
    def __init__(self):
        self._q: queue.Queue[str | None] = queue.Queue()
        self.ready = threading.Event()
        self.thread = threading.Thread(target=self._run, daemon=False)
    def start(self):
        self.thread.start(); self.ready.wait(timeout=5)
    def speak(self, text: str):
        if text: print(f"ğŸ”Š TTS enqueue ({len(text)} chars)"); self._q.put(text)
    def close_and_join(self, drain: bool = True, timeout: float = 30.0):
        try:
            if drain: self._q.join()
            self._q.put(None); self.thread.join(timeout=timeout)
        except Exception: pass
    def _run(self):
        try:
            api_key = _get_env("TYPECAST_API_KEY")
            voice_id = _get_env("TYPECAST_VOICE_ID")
            if not api_key or not voice_id:
                print("â— TYPECAST_API_KEY ë˜ëŠ” TYPECAST_VOICE_IDê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤."); self.ready.set(); return
            model = _get_env("TYPECAST_MODEL", "ssfm-v21")
            language = _get_env("TYPECAST_LANGUAGE", "kor")
            audio_format = _get_env("TYPECAST_AUDIO_FORMAT", "wav")
            emotion = _get_env("TYPECAST_EMOTION", "")
            intensity = float(_get_env("TYPECAST_EMOTION_INTENSITY", "1.0") or "1.0")
            seed_env = _get_env("TYPECAST_SEED", "")
            seed = int(seed_env) if (seed_env and seed_env.isdigit()) else None
            self.ready.set()
            print("â–¶ Typecast TTS ì¤€ë¹„ ì™„ë£Œ")
            url = "https://api.typecast.ai/v1/text-to-speech"
            headers = {"X-API-KEY": api_key, "Content-Type": "application/json"}
            while True:
                item = self._q.get()
                if item is None: self._q.task_done(); break
                try:
                    payload = {"voice_id": voice_id, "text": item, "model": model, "language": language, "output": {"volume": 100, "audio_pitch": 0, "audio_tempo": 1.0, "audio_format": audio_format}}
                    if emotion: payload["prompt"] = {"emotion_preset": emotion, "emotion_intensity": intensity}
                    if seed is not None: payload["seed"] = seed
                    r = requests.post(url, headers=headers, json=payload, timeout=60)
                    if r.status_code == 200:
                        data = r.content
                        with io.BytesIO(data) as buf:
                            with wave.open(buf, "rb") as wf:
                                sr = wf.getframerate(); sampwidth = wf.getsampwidth(); frames = wf.readframes(wf.getnframes())
                        if sampwidth == 2: audio = np.frombuffer(frames, dtype=np.int16).astype(np.float32) / 32768.0
                        else: audio = np.frombuffer(frames, dtype=np.int16).astype(np.float32) / 32768.0
                        sd.play(audio, sr); sd.wait(); print("âœ… TTS done")
                    else: print(f"âŒ Typecast ì˜¤ë¥˜ {r.status_code}: {r.text[:200]}")
                finally: self._q.task_done()
        except Exception as e: print(f"â„¹ï¸ Typecast TTS ìŠ¤ë ˆë“œ ì˜¤ë¥˜: {e}"); self.ready.set()


class PressToTalk:
    def __init__(self,
                 start_dance_cb: Optional[Callable[[], None]] = None,
                 stop_dance_cb: Optional[Callable[[], None]] = None,
                 emotion_queue: Optional[queue.Queue] = None,
                 hotword_queue: Optional[queue.Queue] = None,
                 stop_event: Optional[threading.Event] = None):
        api_key = os.environ.get("GOOGLE_API_KEY")
        if not api_key or not api_key.strip():
            print("â— GOOGLE_API_KEYê°€ ì—†ìŠµë‹ˆë‹¤."); sys.exit(1)

        genai.configure(api_key=api_key)
        self.model = genai.GenerativeModel(MODEL_NAME)
        self.chat = genai.GenerativeModel(MODEL_NAME, system_instruction=SYSTEM_INSTRUCTION).start_chat(history=[])
        self.router_model = genai.GenerativeModel(
            MODEL_NAME,
            system_instruction=(
                "ë„ˆëŠ” ëª…ë ¹ ë¼ìš°í„°ë‹¤. í•œêµ­ì–´ ë¬¸ì¥ì„ ë³´ê³  ì˜ë„ë¥¼ ë¶„ë¥˜í•œë‹¤. "
                "dance=ì‚¬ìš©ìê°€ ì‹¤ì œë¡œ ì¶¤ì„ 'ì‹œì‘í•˜ë¼ê³ ' ëª…ë ¹/ìš”ì²­/ìŠ¹ì¸. "
                "stop=ì¶¤ì„ 'ë©ˆì¶”ë¼'ëŠ” ëª…ë ¹/ìš”ì²­/ìŠ¹ì¸. "
                "chat=ì¼ë°˜ ëŒ€í™”(ì§ˆë¬¸/ì¡ë‹´/ì„¤ëª…/ê°ì •í‘œí˜„/ì¶¤ì— ëŒ€í•œ ê²¬í•´Â·ê°€ì •ì  ì§ˆë¬¸ í¬í•¨). "
                "ë¶€ì •/ê¸ˆì§€/ê±°ì ˆ í‘œí˜„(ì˜ˆ:'ì¶¤ ì¶”ì§€ ë§ˆ','ì¶¤ì€ ì•ˆë¼','ê·¸ë§Œë‘ì§€ ë§ê³  ê³„ì†')ì€ ì •í™•íˆ ë°˜ì˜í•˜ë¼. "
                "ì˜¤ì§ ì•„ë˜ JSONë§Œ ì¶œë ¥:\n"
                '{ "intent": "dance|stop|chat", "normalized_text": "<ì˜ë¯¸ë§Œ ë³´ì¡´í•œ ê°„ê²°í•œ ë¬¸ì¥>", '
                '"speakable_reply": "<ì˜ë„ê°€ chatì¼ ë•Œ 1~2ë¬¸ì¥ ê³µê°í˜• ì§§ì€ ë‹µë³€. dance/stopì´ë©´ ë¹ˆ ë¬¸ìì—´>" }'
            ),
            generation_config={"response_mime_type": "application/json", "temperature": 0.2}
        )
        
        self.start_dance_cb = start_dance_cb
        self.stop_dance_cb  = stop_dance_cb
        self.emotion_queue = emotion_queue
        self.hotword_queue = hotword_queue
        self.stop_event = stop_event or threading.Event()
        
        # â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼ ìˆ˜ì •ëœ ë¶€ë¶„ â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼
        self.last_activity_time = 0
        self.current_listener = None
        # â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²

        default_engine = "sapi" if IS_WINDOWS else "typecast"
        engine = _get_env("TTS_ENGINE", default_engine).lower()
        if engine == "sapi" and not IS_WINDOWS: engine = "typecast"
        if engine == "typecast": self.tts = TypecastTTSWorker()
        else: self.tts = SapiTTSWorker()
        self.tts.start()

        self.state = RecorderState()
        self._print_intro()
        if ENABLE_GREETING:
            self.tts.speak(GREETING_TEXT)
            if self.emotion_queue: self.emotion_queue.put("NEUTRAL")

    def _print_intro(self):
        print("\n=== Gemini PTT (í†µí•© ë²„ì „) ===")
        print("â–¶ 'ì•ˆë…• ëª¨í‹°'ë¡œ í˜¸ì¶œ(SLEEPY ìƒíƒœ) â†’ ìŠ¤í˜ì´ìŠ¤ë°”ë¡œ ëŒ€í™”(NEUTRAL ìƒíƒœ) â†’ ESCë¡œ ì¢…ë£Œ")
        print("â–¶ [User ] ì „ì‚¬ ê²°ê³¼ / [Gemini] ëª¨ë¸ ë‹µë³€")
        print("â–¶ í‚¤ì›Œë“œ: 'ì¶¤' â†’ ëŒ„ìŠ¤ ì‹œì‘ / 'ê·¸ë§Œ' â†’ ëŒ„ìŠ¤ ì •ì§€")
        print(f"â–¶ MODEL={MODEL_NAME}, SR={SAMPLE_RATE}Hz")
        v_id, out_desc = getattr(self.tts, "voice_id", None), getattr(self.tts, "output_device_desc", None)
        if v_id: print(f"â–¶ TTS Voice : {v_id}")
        if out_desc: print(f"â–¶ TTS Output: {out_desc}")
        print("----------------------------------------------------------------\n")

    def _audio_callback(self, indata, frames, time_info, status):
        if status: print(f"[ì˜¤ë””ì˜¤ ê²½ê³ ] {status}", file=sys.stderr)
        self.state.frames_q.put(indata.copy())

    def _start_recording(self):
        if self.state.recording: return
        if self.emotion_queue:
            self.emotion_queue.put("RESET_SLEEPY_TIMER")

        while not self.state.frames_q.empty():
            try: self.state.frames_q.get_nowait()
            except queue.Empty: break
        device_idx = None
        env_dev = os.environ.get("INPUT_DEVICE_INDEX")
        if env_dev and env_dev.strip():
            try: device_idx = int(env_dev.strip())
            except Exception: device_idx = None
        if device_idx is None:
            env_name = os.environ.get("INPUT_DEVICE_NAME", "")
            if env_name: device_idx = _find_input_device_by_name(env_name)
        try:
            if device_idx is not None: dinfo = sd.query_devices(device_idx, 'input')
            else: default_in = sd.default.device[0]; dinfo = sd.query_devices(default_in, 'input')
            print(f"ğŸšï¸  ì…ë ¥ ì¥ì¹˜: {dinfo['name']}")
        except Exception: pass
        self.state.stream = sd.InputStream(samplerate=SAMPLE_RATE, channels=CHANNELS, dtype=DTYPE, callback=self._audio_callback, blocksize=0, device=device_idx)
        self.state.stream.start()
        self.state.recording = True
        print("ğŸ™ï¸  ë…¹ìŒ ì‹œì‘ (ìŠ¤í˜ì´ìŠ¤ë°” ìœ ì§€ ì¤‘)...")

    def _stop_recording_and_transcribe(self):
        if not self.state.recording: return
        print("â¹ï¸  ë…¹ìŒ ì¢…ë£Œ, ì „ì‚¬ ì¤‘...")
        self.state.recording = False
        try:
            if self.state.stream: self.state.stream.stop(); self.state.stream.close()
        finally: self.state.stream = None
        chunks = []
        while not self.state.frames_q.empty(): chunks.append(self.state.frames_q.get())
        if not chunks: print("(ë…¹ìŒ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.)\n"); return
        audio_np = np.concatenate(chunks, axis=0)
        wav_bytes = self._to_wav_bytes(audio_np, SAMPLE_RATE, CHANNELS, DTYPE)
        threading.Thread(target=self._transcribe_then_chat, args=(wav_bytes,), daemon=True).start()

    @staticmethod
    def _to_wav_bytes(audio_np: np.ndarray, samplerate: int, channels: int, dtype: str) -> bytes:
        with io.BytesIO() as buf:
            with wave.open(buf, 'wb') as wf:
                wf.setnchannels(channels); wf.setsampwidth(np.dtype(dtype).itemsize)
                wf.setframerate(samplerate); wf.writeframes(audio_np.tobytes())
            return buf.getvalue()

    def _route_intent(self, text: str) -> dict:
        try:
            resp = self.router_model.generate_content(text)
            raw = _extract_text(resp); data = json.loads(raw)
            if not isinstance(data, dict): raise ValueError("router JSON is not a dict")
            intent = data.get("intent", "chat")
            if intent not in ("dance", "stop", "chat"): intent = "chat"
            return {"intent": intent, "normalized_text": str(data.get("normalized_text", text)), "speakable_reply": str(data.get("speakable_reply", "")) if intent == "chat" else ""}
        except Exception as e:
            print(f"(router í´ë°±) {e}")
            low = text.lower()
            if any(neg in text for neg in ["í•˜ì§€ ë§ˆ", "í•˜ì§€ë§ˆ", "ì•ˆë¼", "ì•ˆ ë¼", "ê·¸ë§Œë‘ì§€ ë§ˆ", "ë©ˆì¶”ì§€ ë§ˆ"]): return {"intent": "chat", "normalized_text": text, "speakable_reply": ""}
            if "ê·¸ë§Œ" in text: return {"intent": "stop", "normalized_text": text, "speakable_reply": ""}
            if "ì¶¤" in text: return {"intent": "dance", "normalized_text": text, "speakable_reply": ""}
            return {"intent": "chat", "normalized_text": text, "speakable_reply": ""}
    
    def _analyze_and_send_emotion(self, text: str):
        if not self.emotion_queue or not text: return
        low_text = text.lower()
        if any(w in low_text for w in ["ì‹ ë‚˜", "ì¬ë°Œ", "ì¢‹ì•„", "í–‰ë³µ", "ìµœê³ "]): self.emotion_queue.put("HAPPY")
        elif any(w in low_text for w in ["ë†€ë¼ìš´", "ë†€ë", "ê¹œì§", "ì„¸ìƒì—"]): self.emotion_queue.put("SURPRISED")
        elif any(w in low_text for w in ["ìŠ¬í¼", "ìš°ìš¸", "í˜ë“¤", "ì†ìƒ"]): self.emotion_queue.put("SAD")
        elif any(w in low_text for w in ["í™”ë‚˜", "ì§œì¦", "ì‹«ì–´", "ìµœì•…"]): self.emotion_queue.put("ANGRY")
        elif any(w in low_text for w in ["ì‚¬ë‘", "ë‹¤ì •", "ë”°ëœ»", "ê³ ë§ˆì›Œ"]): self.emotion_queue.put("TENDER")
        elif any(w in low_text for w in ["ê¶ê¸ˆ", "ìƒê°", "ê¸€ì„", "í .."]): self.emotion_queue.put("THINKING")
        else: self.emotion_queue.put("NEUTRAL")

    def _transcribe_then_chat(self, wav_bytes: bytes):
        try:
            b64 = base64.b64encode(wav_bytes).decode("ascii")
            parts = [{"text": PROMPT_TEXT}, {"inline_data": {"mime_type": "audio/wav", "data": b64}}]
            resp = self.model.generate_content(parts)
            user_text = _extract_text(resp)
            if not user_text: print("ğŸ“ ì „ì‚¬ ê²°ê³¼ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.\n"); return
            ts = datetime.now().strftime("%H:%M:%S"); print(f"[{ts}] [User ] {user_text}")
            route = self._route_intent(user_text)
            intent, model_text, speak_text = route["intent"], "", ""

            if intent == "chat":
                if route.get("speakable_reply"): model_text = route["speakable_reply"]
                else: reply = self.chat.send_message(user_text); model_text = _extract_text(reply) or ""
                speak_text = model_text
                self._analyze_and_send_emotion(model_text) 

            elif intent == "dance":
                print("ğŸ’¡ ì˜ë„: DANCE START")
                if callable(self.start_dance_cb):
                    try: self.start_dance_cb()
                    except Exception as e: print(f"âš ï¸ start_dance_cb ì‹¤í–‰ ì˜¤ë¥˜: {e}")
                
                if self.emotion_queue:
                    chosen_emotion = random.choice(["EXCITED"])
                    self.emotion_queue.put(chosen_emotion)
                    print(f"ğŸ’ƒ ì¶¤ ì‹œì‘! í‘œì •ì„ {chosen_emotion}ë¡œ ë³€ê²½í•©ë‹ˆë‹¤.")

                model_text = "ë„¤! ëª¨í‹°ê°€ ì¶¤ì„ ì¶°ë³¼ê²Œìš”"; speak_text = "ë„¤! ëª¨í‹°ê°€ ì¶¤ì„ ì¶°ë³¼ê²Œìš”"

            elif intent == "stop":
                print("ğŸ’¡ ì˜ë„: DANCE STOP")
                if callable(self.stop_dance_cb):
                    try: self.stop_dance_cb()
                    except Exception as e: print(f"âš ï¸ stop_dance_cb ì‹¤í–‰ ì˜¤ë¥˜: {e}")
                
                if self.emotion_queue:
                    self.emotion_queue.put("NEUTRAL")

                model_text = "(ì¶¤ ì •ì§€ ëª…ë ¹ ì²˜ë¦¬)"

            print(f"[{ts}] [Gemini] {model_text}\n")
            if speak_text: self.tts.speak(speak_text)
            
        except Exception as e: print(f"âŒ ì²˜ë¦¬ ì‹¤íŒ¨: {e}\n")
        finally:
            if self.emotion_queue:
                self.emotion_queue.put("RESET_SLEEPY_TIMER")

    def _on_press(self, key):
        if self.stop_event.is_set(): return False
        try:
            if key == keyboard.Key.space: self._start_recording()
        except Exception as e: print(f"[í‚¤ ì²˜ë¦¬ ì˜¤ë¥˜ on_press] {e}", file=sys.stderr)

    def _on_release(self, key):
        if self.stop_event.is_set(): return False
        try:
            if key == keyboard.Key.space:
                # â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼ ìˆ˜ì •ëœ ë¶€ë¶„ â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼
                self.last_activity_time = time.time()
                # â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²
                self._stop_recording_and_transcribe()
            elif key == keyboard.Key.esc:
                print("ESC ê°ì§€ -> ì¢…ë£Œ ì‹ í˜¸ ë³´ëƒ„")
                if self.current_listener and self.current_listener.is_alive():
                    self.current_listener.stop()
                self.stop_event.set()
                return False 
        except Exception as e: print(f"[í‚¤ ì²˜ë¦¬ ì˜¤ë¥˜ on_release] {e}", file=sys.stderr)

    # â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼ ìˆ˜ì •ëœ ë¶€ë¶„ â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼
    def run(self):
        while not self.stop_event.is_set():
            print("â–¶ 'ì•ˆë…• ëª¨í‹°' í˜¸ì¶œ(SLEEPY ìƒíƒœì—ì„œ)ì„ ê¸°ë‹¤ë¦½ë‹ˆë‹¤... (ì¢…ë£Œ: ESC)")
            try:
                signal = self.hotword_queue.get(timeout=1.0)
                
                if signal == "hotword_detected" and not self.stop_event.is_set():
                    print("ğŸ’¡ í•«ì›Œë“œ ê°ì§€! ëŒ€í™” ì„¸ì…˜ì„ ì‹œì‘í•©ë‹ˆë‹¤.")
                    if self.emotion_queue: self.emotion_queue.put("WAKE")
                    self.tts.speak("ë„¤, ë§ì”€í•˜ì„¸ìš”.")
                    
                    self.last_activity_time = time.time()
                    self.current_listener = keyboard.Listener(on_press=self._on_press, on_release=self._on_release)
                    self.current_listener.start()
                    
                    # 25ì´ˆ(wake 3ì´ˆ + neutral 20ì´ˆ + ì—¬ìœ  2ì´ˆ) ë™ì•ˆ ëŒ€ê¸°
                    while time.time() - self.last_activity_time < 25:
                        if self.stop_event.is_set():
                            break
                        time.sleep(0.1)

                    if self.current_listener.is_alive():
                        self.current_listener.stop()
                    
                    if not self.stop_event.is_set():
                         print("â–¶ ëŒ€í™” ì„¸ì…˜ ì‹œê°„ ì´ˆê³¼. ë‹¤ì‹œ í•«ì›Œë“œ ëŒ€ê¸° ìƒíƒœë¡œ ì „í™˜í•©ë‹ˆë‹¤.")

            except queue.Empty:
                continue
            except (KeyboardInterrupt, SystemExit):
                self.stop_event.set()
                break
        
        print("PTT App ì¢…ë£Œ ì ˆì°¨ ì‹œì‘...")
        if self.current_listener and self.current_listener.is_alive():
            self.current_listener.stop()
        try:
            if FAREWELL_TEXT: self.tts.speak(FAREWELL_TEXT)
        finally:
            self.tts.close_and_join(drain=True)
        print("PTT App ì •ìƒ ì¢…ë£Œ")
    # â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²